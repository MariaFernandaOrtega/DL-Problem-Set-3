# NLP From Scratch: Translation with a Sequence to Sequence Network and Attention


In this project we will be teaching a neural network to translate from German to English.

This is made possible by the simple but powerful idea of the sequence to sequence network, in which two recurrent neural networks work together to transform one sequence to another. An encoder network condenses an input sequence into a vector, and a decoder network unfolds that vector into a new sequence.

To improve upon this model we'll use an attention mechanism, which lets the decoder learn to focus over a specific range of the input sequence.

ðŸ““ Notebook: Problem_Set_3.ipynb â€“ a step-by-step walkthrough of the full process.

ðŸ“‚ Data: /Data â€“ dataset required to run the model.

Contributors: Maria Fernanda Ortega Valencia & Santiago Sordo Ruz
